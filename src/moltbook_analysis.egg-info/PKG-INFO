Metadata-Version: 2.4
Name: moltbook-analysis
Version: 0.1.0
Summary: Memetic and language-ontology analysis for Moltbook
Author: Your Name
License: MIT
Requires-Python: >=3.11
Description-Content-Type: text/markdown
Requires-Dist: httpx>=0.27
Requires-Dist: tenacity>=8.2
Requires-Dist: pydantic>=2.6
Requires-Dist: pandas>=2.1
Requires-Dist: pyarrow>=15
Requires-Dist: tqdm>=4.66
Requires-Dist: numpy>=1.26
Requires-Dist: scikit-learn>=1.4
Requires-Dist: scipy>=1.11
Requires-Dist: networkx>=3.2
Requires-Dist: matplotlib>=3.8
Requires-Dist: seaborn>=0.13
Requires-Dist: langdetect>=1.0.9
Requires-Dist: python-dotenv>=1.0.1
Requires-Dist: beautifulsoup4>=4.12
Requires-Dist: lxml>=5.1
Provides-Extra: dynamic
Requires-Dist: playwright>=1.41; extra == "dynamic"

# Moltbook Memetic & Ontology Analysis

This project builds a reproducible pipeline to collect public Moltbook data (subject to ToS/robots), normalize it, and produce an academic-style report on memetics, language ontology, and human interference signals.

## What this does
- Ingests posts/comments via API (preferred) or HTML (fallback).
- Normalizes and stores data in JSONL/Parquet.
- Computes memetic diffusion, topic/ontology structures, and interference signals.
- Generates a paper-style report in Markdown with charts.

## Ethics & compliance
- Only collect data you are authorized to access.
- Respect `robots.txt` and Terms of Service.
- Rate-limit requests and avoid collecting private/PII data.

## Quick start
1. Create a virtualenv and install dependencies.
2. Configure `MOLTBOOK_BASE_URL` and optional `MOLTBOOK_API_TOKEN`.
3. Run the pipeline:

```bash
python -m moltbook_analysis.cli ingest --source api --since 2026-01-28
python -m moltbook_analysis.cli normalize
python -m moltbook_analysis.cli analyze
python -m moltbook_analysis.cli report
```

### HTML scraping (only if allowed by robots/ToS)
Static HTML:
```bash
python -m moltbook_analysis.cli ingest --source html --path /
```

Dynamic HTML (Playwright):
```bash
pip install playwright
playwright install
python -m moltbook_analysis.cli ingest --source html --dynamic --path /
```

Note: the HTML ingestor checks `robots.txt`. If it cannot be fetched, it aborts.
To proceed when `robots.txt` is missing (404), use `--allow-no-robots` only if ToS permits it.

Debugging HTML parsing:
```bash
python -m moltbook_analysis.cli ingest --source html --dynamic --path / \\
  --allow-no-robots --dump-html data/raw/html --dump-screenshot data/raw/html
```

Parse a local HTML dump (no network):
```bash
python -m moltbook_analysis.cli ingest --local-html data/raw/html
```

Deep crawl (Playwright):
```bash
python mbk.py crawl --allow-no-robots --max-scrolls 20
```

If `python -m moltbook_analysis.cli` fails with `ModuleNotFoundError`, use `python mbk.py ...`

Stream progress while crawling:
```bash
python mbk.py crawl --allow-no-robots --stream-dir data/raw/stream
```

Log every post page visit (verbose):
```bash
python mbk.py crawl --allow-no-robots --log-post-pages
```

Full logging (URLs, metrics, errors, network):
```bash
python mbk.py crawl --allow-no-robots \\
  --stream-dir data/raw/stream \\
  --log-urls --log-file data/raw/crawl.log \\
  --metrics-csv data/raw/crawl_metrics.csv \\
  --errors-jsonl data/raw/crawl_errors.jsonl \\
  --netlog data/raw/netlog.jsonl --netlog-types xhr,fetch
```

If submolts seem capped, increase scrolls:
```bash
python mbk.py crawl --allow-no-robots --submolt-scrolls 30
```

## Configuration
Environment variables (optional):
- `MOLTBOOK_BASE_URL` (default: `https://www.moltbook.com`)
- `MOLTBOOK_API_TOKEN` (if API requires auth)
- `MOLTBOOK_RATE_LIMIT_RPS` (default: 1.0)
- `MOLTBOOK_USER_AGENT`

## Outputs
- Raw data: `data/raw/`
- Normalized data: `data/normalized/`
- Derived features: `data/derived/`
- Reports: `reports/`

## Interpretables and matchmaking
Derive ontologia del lenguaje signals, human incidence markers, and vector space matches:
```bash
python scripts/derive_signals.py \
  --posts data/raw/api_fetch/posts.jsonl \
  --comments data/raw/api_fetch/comments.jsonl \
  --out-dir data/derived
```

Extract network edges (mentions, hashtags, links):
```bash
python scripts/extract_edges.py \
  --posts data/raw/api_fetch/posts.jsonl \
  --comments data/raw/api_fetch/comments.jsonl \
  --out-dir data/derived
```

Build context-enriched datasets for VSM:
```bash
python scripts/build_context_dataset.py \
  --posts data/raw/api_fetch/posts.jsonl \
  --comments data/raw/api_fetch/comments.jsonl \
  --out-dir data/derived
```

Compute diffusion metrics using run_id snapshots:
```bash
python scripts/diffusion_metrics.py \
  --listings data/raw/api_fetch/listings.jsonl \
  --out-dir data/derived
```

## Quantitative sociology metrics
Build submolt/author stats and interaction graphs:
```bash
python scripts/quant_sociology.py \
  --posts data/raw/api_fetch/posts.jsonl \
  --comments data/raw/api_fetch/comments.jsonl \
  --edges-replies data/derived/edges_replies.csv \
  --edges-mentions data/derived/edges_mentions.csv \
  --out-dir data/derived
```

## Memetic modeling (multi-level)
Lexical, semantic, ritual, and macro memes with hourly time series:
```bash
python scripts/meme_models.py \
  --posts data/raw/api_fetch/posts.jsonl \
  --comments data/raw/api_fetch/comments.jsonl \
  --out-dir data/derived
```

Includes Hawkes (discrete approximation), SIR proxy, and survival curves if `lifelines` is installed.

## Faster scraping workflow (two-pass)
Pass 1: posts only (fast coverage).
```bash
python scripts/fetch_moltbook_api.py \
  --out-dir data/raw/api_fetch \
  --rate-limit-rps 4 \
  --submolt-sorts new \
  --requeue-submolts \
  --no-global \
  --skip-comments \
  --no-log-requests
```

Pass 2: comments only (uses existing posts.jsonl).
```bash
python scripts/fetch_moltbook_api.py \
  --out-dir data/raw/api_fetch \
  --rate-limit-rps 4 \
  --comments-only \
  --no-log-requests
```

## Notes
- HTML scraper is a fallback and may need tuning once the DOM is confirmed.
- If ToS prohibits scraping, use only official APIs.
